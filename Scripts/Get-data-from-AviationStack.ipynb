{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bd_Ou-CYthl"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import requests  # For making HTTP requests to external APIs\n",
        "import json  # For handling JSON data\n",
        "import pandas as pd  # For data manipulation and processing\n",
        "import boto3  # AWS SDK for Python to interact with AWS services\n",
        "import uuid  # For generating unique identifiers\n",
        "from datetime import datetime  # For handling datetime objects\n",
        "\n",
        "# Constants - These values should be configured as per your environment\n",
        "S3_BUCKET = 'S3_BUCKET_NAME'  # Name of the S3 bucket where data will be stored\n",
        "S3_KEY = 'offset.json'  # S3 object key for the offset file (tracking the last processed record)\n",
        "KINESIS_STREAM_NAME = 'DATA_STREAM_NAME'  # The name of the Kinesis stream for data ingestion\n",
        "API_ACCESS_KEY = 'YOUR_API_KEY'  # API access key for authenticating API requests\n",
        "AIRLINE_NAME = 'American Airlines'  # The airline for which the data is being tracked\n",
        "BATCH_SIZE = 100  # The number of records to fetch per API call\n",
        "\n",
        "# Initialize AWS clients\n",
        "# Boto3 client to interact with Amazon S3\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# Boto3 client to interact with Amazon Kinesis for real-time data streaming\n",
        "kinesis = boto3.client('kinesis')\n",
        "\n",
        "# Functionality that interacts with the defined constants will be implemented below this point\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_partition_key():\n",
        "    \"\"\"\n",
        "    Generate a unique partition key using UUID.\n",
        "\n",
        "    This function generates a universally unique identifier (UUID) and returns it\n",
        "    as a string to be used as a partition key in streaming services like Kinesis.\n",
        "\n",
        "    Returns:\n",
        "        str: A string representation of a UUID to be used as a partition key.\n",
        "    \"\"\"\n",
        "    return str(uuid.uuid4())  # Generate a UUID and convert it to a string\n"
      ],
      "metadata": {
        "id": "1xjcjIloY7AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_offset_data():\n",
        "    \"\"\"\n",
        "    Retrieve the current offset data from S3.\n",
        "\n",
        "    This function reads the offset file from S3 to track the last processed record.\n",
        "    It ensures the offset is reset if the current date has changed since the last run.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the current date, offset, and total records.\n",
        "              If the offset file does not exist or an error occurs, it initializes\n",
        "              a new offset structure starting from zero.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Attempt to fetch the offset JSON file from S3\n",
        "        obj = s3.get_object(Bucket=S3_BUCKET, Key=S3_KEY)\n",
        "\n",
        "        # Read and decode the file content\n",
        "        data = json.loads(obj['Body'].read().decode())\n",
        "\n",
        "        # Get today's date in 'YYYY-MM-DD' format\n",
        "        current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "        # If the date has changed, reset the offset and total_records\n",
        "        if data.get('current_date') != current_date:\n",
        "            data = {\n",
        "                'current_date': current_date,\n",
        "                'offset': 0,\n",
        "                'total_records': None\n",
        "            }\n",
        "\n",
        "        return data\n",
        "\n",
        "    except s3.exceptions.NoSuchKey:\n",
        "        # If the offset file doesn't exist, start fresh with default values\n",
        "        return {\n",
        "            'current_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "            'offset': 0,\n",
        "            'total_records': None\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log the exception and return a fresh offset structure\n",
        "        print(f\"Error reading offset from S3: {e}\")\n",
        "        return {\n",
        "            'current_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "            'offset': 0,\n",
        "            'total_records': None\n",
        "        }\n"
      ],
      "metadata": {
        "id": "YUXT3Y9YY8yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_offset_data(offset, total_records):\n",
        "    \"\"\"\n",
        "    Update the offset file in S3 with the latest processed offset and total record count.\n",
        "\n",
        "    This function creates or overwrites the offset JSON file in the specified S3 bucket\n",
        "    to persist the current processing state. It's useful for batch or stream processing\n",
        "    to ensure resumability and avoid reprocessing records.\n",
        "\n",
        "    Args:\n",
        "        offset (int): The current offset (index) of the last processed record.\n",
        "        total_records (int or None): The total number of records expected (optional).\n",
        "    \"\"\"\n",
        "    data = {\n",
        "        'current_date': datetime.now().strftime('%Y-%m-%d'),  # Today's date to track date-wise batches\n",
        "        'offset': offset,  # Last processed offset\n",
        "        'total_records': total_records  # Optional: total expected records from API\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Upload the updated offset data as a JSON object to S3\n",
        "        s3.put_object(\n",
        "            Bucket=S3_BUCKET,\n",
        "            Key=S3_KEY,\n",
        "            Body=json.dumps(data)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Log the error for debugging; consider using CloudWatch logs in production\n",
        "        print(f\"Error writing offset to S3: {e}\")\n"
      ],
      "metadata": {
        "id": "_TgWmXCTY_h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_flight_data(offset):\n",
        "    \"\"\"\n",
        "    Fetch a batch of flight data from the AviationStack API.\n",
        "\n",
        "    This function constructs a request URL with parameters such as API key,\n",
        "    airline name, batch size, and offset. It then sends a GET request to fetch\n",
        "    flight data and returns it as a pandas DataFrame along with pagination info.\n",
        "\n",
        "    Args:\n",
        "        offset (int): The offset value to start fetching records from (used for pagination).\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            pd.DataFrame: A DataFrame containing the flight data for the current batch.\n",
        "            dict or None: Pagination metadata from the API response if available.\n",
        "    \"\"\"\n",
        "    # Construct the API URL with query parameters\n",
        "    url = (\n",
        "        f\"https://api.aviationstack.com/v1/flights\"\n",
        "        f\"?access_key={API_ACCESS_KEY}\"\n",
        "        f\"&airline_name={AIRLINE_NAME}\"\n",
        "        f\"&limit={BATCH_SIZE}\"\n",
        "        f\"&offset={offset}\"\n",
        "    )\n",
        "\n",
        "    # Send the GET request to the AviationStack API\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Raise an exception if the response status code is not 200 (OK)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(\n",
        "            f\"API request failed with status {response.status_code}: {response.text}\"\n",
        "        )\n",
        "\n",
        "    # Parse the JSON response body\n",
        "    response_data = response.json()\n",
        "\n",
        "    # Check if 'data' field is present and non-empty\n",
        "    if 'data' not in response_data or not response_data['data']:\n",
        "        # Return an empty DataFrame and pagination info (can be None)\n",
        "        return pd.DataFrame(), response_data.get('pagination', None)\n",
        "\n",
        "    # Normalize nested JSON structure into a flat pandas DataFrame\n",
        "    df = pd.json_normalize(response_data['data'], sep='_')\n",
        "\n",
        "    # Return the DataFrame and pagination metadata (useful for checking if more data exists)\n",
        "    return df, response_data.get('pagination', None)\n"
      ],
      "metadata": {
        "id": "a9aBpHzyZB8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_schema(df):\n",
        "    \"\"\"\n",
        "    Validate the schema and data integrity of the flight DataFrame.\n",
        "\n",
        "    This function ensures that:\n",
        "    - All expected columns are present.\n",
        "    - Critical fields have correct data types.\n",
        "    - Required fields do not contain null values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing flight data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any required column is missing, has invalid types,\n",
        "                    or contains null values in critical fields.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the columns expected in the incoming data\n",
        "    expected_columns = ['flight_iata', 'departure', 'arrival', 'status', 'airline_name']\n",
        "\n",
        "    # Check for missing columns in the DataFrame\n",
        "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        raise ValueError(f\"Missing expected columns: {', '.join(missing_columns)}\")\n",
        "\n",
        "    # Validate that 'flight_iata' contains only strings\n",
        "    if not df['flight_iata'].apply(lambda x: isinstance(x, str)).all():\n",
        "        raise ValueError(\"Column 'flight_iata' must contain only strings.\")\n",
        "\n",
        "    # Ensure 'departure' field is not null (could represent missing location/times)\n",
        "    if df['departure'].isnull().any():\n",
        "        raise ValueError(\"Column 'departure' contains null values, which are not allowed.\")\n",
        "\n",
        "    # Ensure 'arrival' field is not null (important for flight tracking)\n",
        "    if df['arrival'].isnull().any():\n",
        "        raise ValueError(\"Column 'arrival' contains null values, which are not allowed.\")\n",
        "\n",
        "    # If all checks pass, log success\n",
        "    print(\"Schema validation passed.\")\n"
      ],
      "metadata": {
        "id": "nRSghzOMZDuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main orchestration function to extract, validate, and stream flight data.\n",
        "\n",
        "    This function:\n",
        "    - Retrieves the last saved offset from S3.\n",
        "    - Fetches flight data from the AviationStack API in batches.\n",
        "    - Validates each batch against a defined schema.\n",
        "    - Sends validated records to a Kinesis Data Stream.\n",
        "    - Updates the offset metadata in S3 to ensure idempotency and resumption.\n",
        "\n",
        "    Runs in a loop until all available records are processed or an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load current offset and total record metadata from S3\n",
        "    offset_data = get_offset_data()\n",
        "    current_offset = offset_data['offset']\n",
        "    total_records = offset_data['total_records']\n",
        "\n",
        "    while True:\n",
        "        print(f\"Fetching data with offset {current_offset}...\")\n",
        "\n",
        "        try:\n",
        "            # Fetch batch of flight data from API\n",
        "            df, pagination = fetch_flight_data(current_offset)\n",
        "        except Exception as e:\n",
        "            # Break loop if API fetch fails\n",
        "            print(f\"Failed to fetch data: {e}\")\n",
        "            break\n",
        "\n",
        "        # Stop processing if no more data or pagination info is missing\n",
        "        if df.empty or not pagination:\n",
        "            print(\"No more data to process or pagination info missing.\")\n",
        "            break\n",
        "\n",
        "        # Validate schema of incoming data\n",
        "        try:\n",
        "            validate_schema(df)\n",
        "        except ValueError as e:\n",
        "            print(f\"Schema validation failed: {e}\")\n",
        "            break\n",
        "\n",
        "        # Set total_records only once, based on the first pagination response\n",
        "        if total_records is None:\n",
        "            total_records = pagination.get('total', None)\n",
        "            if total_records is None:\n",
        "                print(\"Total records info missing from API response.\")\n",
        "                break\n",
        "\n",
        "        # Check if remaining records are fewer than a full batch\n",
        "        remaining = total_records - current_offset\n",
        "        if remaining < BATCH_SIZE:\n",
        "            print(f\"Not enough data available to fetch next batch. Remaining: {remaining}\")\n",
        "            break\n",
        "\n",
        "        # Convert DataFrame to list of JSON records for streaming\n",
        "        records = df.to_dict(orient='records')\n",
        "\n",
        "        # Send each record to Kinesis stream\n",
        "        for record in records:\n",
        "            try:\n",
        "                kinesis.put_record(\n",
        "                    StreamName=KINESIS_STREAM_NAME,\n",
        "                    Data=json.dumps(record) + '\\n',\n",
        "                    PartitionKey=get_partition_key()\n",
        "                )\n",
        "                print(f\"Successfully sent record with flight number: {record.get('flight_iata', 'N/A')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to send record: {record}. Error: {e}\")\n",
        "\n",
        "        # Increment offset and update metadata in S3\n",
        "        current_offset += BATCH_SIZE\n",
        "        update_offset_data(current_offset, total_records)\n",
        "\n",
        "        # Break loop if all records have been processed\n",
        "        if current_offset >= total_records:\n",
        "            print(\"Reached end of available records.\")\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6ldaY5wFZFnH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}