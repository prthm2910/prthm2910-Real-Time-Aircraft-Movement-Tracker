{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys  # Used to access arguments passed to the Glue job at runtime\n",
        "\n",
        "# AWS Glue-specific libraries\n",
        "from awsglue.transforms import *  # Provides built-in Glue transformations like ApplyMapping, Join, etc.\n",
        "from awsglue.utils import getResolvedOptions  # Used to retrieve job arguments passed from Glue job parameters\n",
        "from awsglue.context import GlueContext  # Glue-specific context wrapper for SparkContext\n",
        "from awsglue.job import Job  # Provides structure for defining and managing the Glue job lifecycle\n",
        "from awsglue.gluetypes import *  # Contains Glue-compatible data types for schema definitions\n",
        "\n",
        "# Spark context initialization\n",
        "from pyspark.context import SparkContext  # Entry point for Spark functionality in Glue\n",
        "\n",
        "# AWS Glue Data Quality libraries\n",
        "from awsgluedq.transforms import EvaluateDataQuality  # Used to run DQ rulesets and evaluate data quality\n",
        "\n",
        "# DynamicFrame abstraction used in AWS Glue\n",
        "from awsglue import DynamicFrame  # Glue-specific abstraction over Spark DataFrames with built-in transformations\n"
      ],
      "metadata": {
        "id": "4LeBmDz4ejDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_null_fields(ctx, schema, path, output, nullStringSet, nullIntegerSet, frame):\n",
        "    \"\"\"\n",
        "    Recursively traverses the schema of a Glue DynamicFrame and identifies fields\n",
        "    that are effectively \"null\" based on domain-specific null value sets.\n",
        "\n",
        "    Args:\n",
        "        ctx (GlueContext): The Glue context.\n",
        "        schema (StructType): The schema of the data being analyzed.\n",
        "        path (str): Current dot-separated path to the field.\n",
        "        output (list): Accumulator list to collect null field paths.\n",
        "        nullStringSet (set): Set of string values considered null (e.g., {\"\", \"null\", \"none\"}).\n",
        "        nullIntegerSet (set): Set of numeric values considered null (e.g., {0, -1}).\n",
        "        frame (DynamicFrame): The Glue DynamicFrame containing the data.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of field paths (str) that are considered null-like fields.\n",
        "    \"\"\"\n",
        "\n",
        "    # If current schema is a struct, recursively evaluate each nested field\n",
        "    if isinstance(schema, StructType):\n",
        "        for field in schema:\n",
        "            new_path = path + \".\" if path != \"\" else path\n",
        "            output = _find_null_fields(\n",
        "                ctx,\n",
        "                field.dataType,\n",
        "                new_path + field.name,\n",
        "                output,\n",
        "                nullStringSet,\n",
        "                nullIntegerSet,\n",
        "                frame\n",
        "            )\n",
        "\n",
        "    # If current schema is an array of structs, recursively evaluate struct elementType\n",
        "    elif isinstance(schema, ArrayType):\n",
        "        if isinstance(schema.elementType, StructType):\n",
        "            output = _find_null_fields(\n",
        "                ctx,\n",
        "                schema.elementType,\n",
        "                path,\n",
        "                output,\n",
        "                nullStringSet,\n",
        "                nullIntegerSet,\n",
        "                frame\n",
        "            )\n",
        "\n",
        "    # Directly append null paths for fields explicitly marked NullType\n",
        "    elif isinstance(schema, NullType):\n",
        "        output.append(path)\n",
        "\n",
        "    else:\n",
        "        # Convert DynamicFrame to DataFrame for column analysis\n",
        "        df = frame.toDF()\n",
        "        distinct_set = set()\n",
        "\n",
        "        # Extract distinct values for the field\n",
        "        for row in df.select(path).distinct().collect():\n",
        "            value = row[path.split('.')[-1]]\n",
        "\n",
        "            # Normalize and clean values depending on type\n",
        "            if isinstance(value, list):\n",
        "                # Flatten list of strings or other primitives\n",
        "                distinct_set |= set([item.strip() if isinstance(item, str) else item for item in value])\n",
        "            elif isinstance(value, str):\n",
        "                distinct_set.add(value.strip())\n",
        "            else:\n",
        "                distinct_set.add(value)\n",
        "\n",
        "        # Check if the set of distinct values is a subset of null indicators\n",
        "        if isinstance(schema, StringType):\n",
        "            if distinct_set.issubset(nullStringSet):\n",
        "                output.append(path)\n",
        "        elif isinstance(schema, (IntegerType, LongType, DoubleType)):\n",
        "            if distinct_set.issubset(nullIntegerSet):\n",
        "                output.append(path)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "p7_BjzpDenl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_nulls(glueContext, frame, nullStringSet, nullIntegerSet, transformation_ctx) -> DynamicFrame:\n",
        "    \"\"\"\n",
        "    Identifies and removes fields from a Glue DynamicFrame that are deemed 'null-like',\n",
        "    based on domain-specific null value sets for strings and numeric types.\n",
        "\n",
        "    Args:\n",
        "        glueContext (GlueContext): The AWS Glue context for job execution.\n",
        "        frame (DynamicFrame): The input DynamicFrame to process.\n",
        "        nullStringSet (set): Set of string values considered as null indicators (e.g., {\"\", \"null\", \"none\"}).\n",
        "        nullIntegerSet (set): Set of integer values considered as null indicators (e.g., {0, -1}).\n",
        "        transformation_ctx (str): A unique context string used for job lineage tracking in AWS Glue.\n",
        "\n",
        "    Returns:\n",
        "        DynamicFrame: A new DynamicFrame with null-like fields removed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Identify all null-like fields based on schema and value sets\n",
        "    nullColumns = _find_null_fields(\n",
        "        frame.glue_ctx,        # Glue context (accessed from the frame itself)\n",
        "        frame.schema(),        # Schema of the DynamicFrame\n",
        "        \"\",                    # Starting path (top level)\n",
        "        [],                    # Initial output list to hold null field paths\n",
        "        nullStringSet,         # Domain-specific null-like string values\n",
        "        nullIntegerSet,        # Domain-specific null-like integer values\n",
        "        frame                  # The data frame being analyzed\n",
        "    )\n",
        "\n",
        "    # Step 2: Drop the identified null-like fields using DropFields transformation\n",
        "    return DropFields.apply(\n",
        "        frame=frame,\n",
        "        paths=nullColumns,\n",
        "        transformation_ctx=transformation_ctx  # For Glue lineage tracking\n",
        "    )\n"
      ],
      "metadata": {
        "id": "GUeSP7sxepkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:\n",
        "    \"\"\"\n",
        "    Executes a Spark SQL query on registered temporary views and returns the result as a DynamicFrame.\n",
        "\n",
        "    Args:\n",
        "        glueContext (GlueContext): AWS Glue context required to convert Spark DataFrames back to DynamicFrames.\n",
        "        query (str): The SQL query to execute on the temporary views.\n",
        "        mapping (dict): A dictionary mapping table aliases (used in SQL) to Glue DynamicFrames.\n",
        "        transformation_ctx (str): A unique string used for job tracking and lineage in AWS Glue.\n",
        "\n",
        "    Returns:\n",
        "        DynamicFrame: The result of the SQL query converted back into a DynamicFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    # Register each input DynamicFrame as a temporary SQL view\n",
        "    for alias, frame in mapping.items():\n",
        "        frame.toDF().createOrReplaceTempView(alias)\n",
        "\n",
        "    # Execute the SQL query\n",
        "    result = spark.sql(query)\n",
        "\n",
        "    # Convert the resulting Spark DataFrame back into a Glue DynamicFrame\n",
        "    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)\n",
        "\n",
        "\n",
        "# ---- Glue Job Initialization Section ---- #\n",
        "\n",
        "# Parse job arguments (e.g., --JOB_NAME) passed at runtime\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "\n",
        "# Initialize the Spark context and Glue context\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "\n",
        "# Create the Spark session for SQL operations\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Initialize the Glue job with the given name\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "\n",
        "# ---- Data Quality Default Ruleset ---- #\n",
        "\n",
        "# This is a default data quality ruleset definition.\n",
        "# Additional rules can be appended to this string as needed per dataset.\n",
        "DEFAULT_DATA_QUALITY_RULESET = \"\"\"\n",
        "    Rules = [\n",
        "        ColumnCount > 0\n",
        "    ]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "60i7as-QesF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node airline_raw_data\n",
        "# This step loads raw data from the AWS Glue Data Catalog, where \"ad_raw_data\" is the table in the \"airline_raw_data_db\" database.\n",
        "# This dynamic frame acts as a foundational source for processing the flight data.\n",
        "airline_raw_data_node1744438827213 = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"airline_raw_data_db\", table_name=\"ad_raw_data\", transformation_ctx=\"airline_raw_data_node1744438827213\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "a8VVsk0PexXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Drop Null Fields\n",
        "# This transformation step drops rows with null or missing values for specific fields to ensure that the data being processed is clean.\n",
        "# It helps in maintaining data quality and avoids errors in downstream processing that could arise from missing critical values.\n",
        "# The nullStringSet defines values considered as \"null\", such as empty strings or \"NaN\".\n",
        "DropNullFields_node1744439376908 = drop_nulls(\n",
        "    glueContext,\n",
        "    frame=airline_raw_data_node1744438827213,\n",
        "    nullStringSet={\"\", \"null\", \"NaN\"},\n",
        "    nullIntegerSet={},\n",
        "    transformation_ctx=\"DropNullFields_node1744439376908\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bGQ19ApMgEHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Drop Fields\n",
        "# This transformation removes non-essential fields that are not required for further analysis.\n",
        "# Dropping fields like flight codeshare details and partition fields helps reduce storage and improve processing performance.\n",
        "# In a production environment, this ensures the data remains streamlined and only relevant data is passed on.\n",
        "DropFields_node1744453341635 = DropFields.apply(\n",
        "    frame=DropNullFields_node1744439376908,\n",
        "    paths=[\n",
        "        \"departure_estimated\", \"departure_estimated_runway\", \"departure_actual_runway\",\n",
        "        \"arrival_estimated\", \"arrival_estimated_runway\", \"arrival_actual_runway\",\n",
        "        \"flight_codeshared_airline_name\", \"flight_codeshared_airline_iata\", \"flight_codeshared_airline_icao\",\n",
        "        \"flight_codeshared_flight_number\", \"flight_codeshared_flight_iata\", \"flight_codeshared_flight_icao\",\n",
        "        \"flight_codeshared\", \"partition_1\", \"partition_2\", \"partition_3\", \"partition_0\",\n",
        "        \"airline_name\", \"airline_iata\", \"airline_icao\"\n",
        "    ],\n",
        "    transformation_ctx=\"DropFields_node1744453341635\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "pah9LWmggFoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node fill arrival_actual and departure_actual\n",
        "# This SQL query is designed to replace null values in 'departure_actual' and 'arrival_actual' columns with the corresponding 'scheduled' times.\n",
        "# This transformation is essential for ensuring data consistency, especially when the actual departure/arrival data is missing.\n",
        "# It fills gaps in the data so that later analysis or downstream jobs aren't disrupted by missing values.\n",
        "SqlQuery0 = '''\n",
        "SELECT\n",
        "*,\n",
        "    CASE\n",
        "        WHEN departure_actual IS NULL THEN departure_scheduled\n",
        "        ELSE departure_actual\n",
        "    END AS departure_actual,\n",
        "    CASE\n",
        "        WHEN arrival_actual IS NULL THEN arrival_scheduled\n",
        "        ELSE arrival_actual\n",
        "    END AS arrival_actual\n",
        "FROM american_airlines;\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "zRrRxbxCgHUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the SQL query on the transformed data. The query maps to the dynamic frame of the previous step,\n",
        "# ensuring that the missing 'departure_actual' and 'arrival_actual' values are handled.\n",
        "# This is a critical step to keep the dataset consistent and ready for further processing or analytics.\n",
        "fillarrival_actualanddeparture_actual_node1744452588102 = sparkSqlQuery(\n",
        "    glueContext, query=SqlQuery0, mapping={\"american_airlines\": DropFields_node1744453341635}, transformation_ctx=\"fillarrival_actualanddeparture_actual_node1744452588102\"\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xkKU2yRCgJNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Fix Data Types\n",
        "# This step applies data type corrections to ensure that the columns have the appropriate types for further processing.\n",
        "# Correcting data types ensures data consistency and prevents errors in downstream operations such as database inserts or analytics.\n",
        "FixDataTypes_node1744455298016 = ApplyMapping.apply(\n",
        "    frame=fillarrival_actualanddeparture_actual_node1744452588102,\n",
        "    mappings=[\n",
        "        (\"flight_date\", \"string\", \"flight_date\", \"date\"),\n",
        "        (\"flight_status\", \"string\", \"flight_status\", \"string\"),\n",
        "        (\"departure_airport\", \"string\", \"departure_airport\", \"string\"),\n",
        "        (\"departure_timezone\", \"string\", \"departure_timezone\", \"string\"),\n",
        "        (\"departure_iata\", \"string\", \"departure_iata\", \"string\"),\n",
        "        (\"departure_icao\", \"string\", \"departure_icao\", \"string\"),\n",
        "        (\"departure_terminal\", \"string\", \"departure_terminal\", \"string\"),\n",
        "        (\"departure_gate\", \"string\", \"departure_gate\", \"string\"),\n",
        "        (\"departure_delay\", \"double\", \"departure_delay\", \"int\"),\n",
        "        (\"departure_scheduled\", \"string\", \"departure_scheduled\", \"timestamp\"),\n",
        "        (\"departure_actual\", \"string\", \"departure_actual\", \"timestamp\"),\n",
        "        (\"arrival_airport\", \"string\", \"arrival_airport\", \"string\"),\n",
        "        (\"arrival_timezone\", \"string\", \"arrival_timezone\", \"string\"),\n",
        "        (\"arrival_iata\", \"string\", \"arrival_iata\", \"string\"),\n",
        "        (\"arrival_icao\", \"string\", \"arrival_icao\", \"string\"),\n",
        "        (\"arrival_terminal\", \"string\", \"arrival_terminal\", \"string\"),\n",
        "        (\"arrival_gate\", \"string\", \"arrival_gate\", \"string\"),\n",
        "        (\"arrival_baggage\", \"string\", \"arrival_baggage\", \"string\"),\n",
        "        (\"arrival_scheduled\", \"string\", \"arrival_scheduled\", \"timestamp\"),\n",
        "        (\"arrival_delay\", \"double\", \"arrival_delay\", \"int\"),\n",
        "        (\"arrival_actual\", \"string\", \"arrival_actual\", \"timestamp\"),\n",
        "        (\"flight_number\", \"string\", \"flight_number\", \"int\"),\n",
        "        (\"flight_iata\", \"string\", \"flight_iata\", \"string\"),\n",
        "        (\"flight_icao\", \"string\", \"flight_icao\", \"string\")\n",
        "    ],\n",
        "    transformation_ctx=\"FixDataTypes_node1744455298016\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "nxalnNo5gKxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node flights\n",
        "# The 'SplitFields' transformation is used to divide the dataset into smaller dynamic frames based on specific fields.\n",
        "# Here, we are isolating flight-level information (flight number, date, and status) from other aspects of the data.\n",
        "# This step ensures that the flight-level data can be processed separately from departures and arrivals for further analysis.\n",
        "flights_node1744464879178 = SplitFields.apply(\n",
        "    frame=FixDataTypes_node1744455298016,\n",
        "    paths=[\"flight_number\", \"flight_date\", \"flight_status\", \"flight_iata\", \"flight_icao\"],\n",
        "    name2=\"flights_node17444648791781\",\n",
        "    name1=\"flights_node17444648791780\",\n",
        "    transformation_ctx=\"flights_node1744464879178\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UKC8kAeHgMmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node departures\n",
        "# Similarly, we split the departures information into a separate dynamic frame.\n",
        "# This step is crucial to ensure that we can handle departure-specific details like gate, terminal, delay, and scheduled times independently.\n",
        "departures_node1744466056302 = SplitFields.apply(\n",
        "    frame=FixDataTypes_node1744455298016,\n",
        "    paths=[\"flight_number\", \"departure_airport\", \"departure_timezone\", \"departure_iata\", \"departure_icao\",\n",
        "           \"departure_terminal\", \"departure_gate\", \"departure_delay\", \"departure_scheduled\", \"departure_actual\"],\n",
        "    name2=\"departures_node17444660563021\",\n",
        "    name1=\"departures_node17444660563020\",\n",
        "    transformation_ctx=\"departures_node1744466056302\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "UEv-f1yxgOLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node arrivals\n",
        "# This splits the arrival-specific information, enabling further separate processing for arrival details such as baggage, gate, and delays.\n",
        "arrivals_node1744477694485 = SplitFields.apply(\n",
        "    frame=FixDataTypes_node1744455298016,\n",
        "    paths=[\"flight_number\", \"arrival_airport\", \"arrival_timezone\", \"arrival_iata\", \"arrival_icao\",\n",
        "           \"arrival_terminal\", \"arrival_gate\", \"arrival_baggage\", \"arrival_scheduled\", \"arrival_delay\", \"arrival_actual\"],\n",
        "    name2=\"arrivals_node17444776944851\",\n",
        "    name1=\"arrivals_node17444776944850\",\n",
        "    transformation_ctx=\"arrivals_node1744477694485\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "zaOGfM-agPfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node flights_convert_to_single_dynamic_frame\n",
        "# This step flattens the flight-related data (from the split) back into a single dynamic frame for ease of output storage.\n",
        "# This ensures we have a properly structured frame for writing to S3.\n",
        "flights_convert_to_single_dynamic_frame_node1744465833856 = SelectFromCollection.apply(\n",
        "    dfc=flights_node1744464879178,\n",
        "    key=list(flights_node1744464879178.keys())[0],\n",
        "    transformation_ctx=\"flights_convert_to_single_dynamic_frame_node1744465833856\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "osQ4q3gIgROd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node departures_convert_to_single_dynamic_frame\n",
        "# Similar to the flights conversion, this step combines departure-related data back into a single dynamic frame.\n",
        "# This is crucial for output storage, as well as enabling efficient downstream processing.\n",
        "departures_convert_to_single_dynamic_frame_node1744477596454 = SelectFromCollection.apply(\n",
        "    dfc=departures_node1744466056302,\n",
        "    key=list(departures_node1744466056302.keys())[0],\n",
        "    transformation_ctx=\"departures_convert_to_single_dynamic_frame_node1744477596454\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "twSqf6DogSy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node arrivals_convert_to_single_dynamic_frame\n",
        "# This step flattens the arrival-related data into a single dynamic frame for easy output handling.\n",
        "arrivals_convert_to_single_dynamic_frame_node1744477730878 = SelectFromCollection.apply(\n",
        "    dfc=arrivals_node1744477694485,\n",
        "    key=list(arrivals_node1744477694485.keys())[0],\n",
        "    transformation_ctx=\"arrivals_convert_to_single_dynamic_frame_node1744477730878\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ex77LRo2gUae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Amazon S3 - Flights Data Quality Evaluation\n",
        "# Data Quality evaluation is done at each step to ensure the data meets predefined standards.\n",
        "# This is crucial in a production environment to catch any inconsistencies or errors before writing to S3.\n",
        "EvaluateDataQuality().process_rows(\n",
        "    frame=flights_convert_to_single_dynamic_frame_node1744465833856,\n",
        "    ruleset=DEFAULT_DATA_QUALITY_RULESET,\n",
        "    publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1744462146370\",\n",
        "                        \"enableDataQualityResultsPublishing\": True},\n",
        "    additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "RH85LzNKgWJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the clean, processed flight data to S3 in Parquet format with Snappy compression.\n",
        "AmazonS3_node1744465688208 = glueContext.write_dynamic_frame.from_options(\n",
        "    frame=flights_convert_to_single_dynamic_frame_node1744465833856,\n",
        "    connection_type=\"s3\",\n",
        "    format=\"glueparquet\",\n",
        "    connection_options={\"path\": \"s3://airline-dataset-quaser/ad_cleaned_data/ad_flights/\", \"partitionKeys\": []},\n",
        "    format_options={\"compression\": \"snappy\"},\n",
        "    transformation_ctx=\"AmazonS3_node1744465688208\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "D-9-ikbxgYA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Amazon S3 - Departures Data Quality Evaluation\n",
        "# Similar to flights, data quality checks are performed on departures data before storage in S3.\n",
        "EvaluateDataQuality().process_rows(\n",
        "    frame=departures_convert_to_single_dynamic_frame_node1744477596454,\n",
        "    ruleset=DEFAULT_DATA_QUALITY_RULESET,\n",
        "    publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1744477388388\",\n",
        "                        \"enableDataQualityResultsPublishing\": True},\n",
        "    additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "cO15e0aQgZsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the cleaned departure data to S3 with Snappy compression.\n",
        "AmazonS3_node1744477630245 = glueContext.write_dynamic_frame.from_options(\n",
        "    frame=departures_convert_to_single_dynamic_frame_node1744477596454,\n",
        "    connection_type=\"s3\",\n",
        "    format=\"glueparquet\",\n",
        "    connection_options={\"path\": \"s3://airline-dataset-quaser/ad_cleaned_data/ad_departures/\", \"partitionKeys\": []},\n",
        "    format_options={\"compression\": \"snappy\"},\n",
        "    transformation_ctx=\"AmazonS3_node1744477630245\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hh0aUCE7gbYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Script generated for node Amazon S3 - Arrivals Data Quality Evaluation\n",
        "# A similar data quality evaluation for the arrival data is conducted before writing it to S3.\n",
        "EvaluateDataQuality().process_rows(\n",
        "    frame=arrivals_convert_to_single_dynamic_frame_node1744477730878,\n",
        "    ruleset=DEFAULT_DATA_QUALITY_RULESET,\n",
        "    publishing_options={\"dataQualityEvaluationContext\": \"EvaluateDataQuality_node1744477388388\",\n",
        "                        \"enableDataQualityResultsPublishing\": True},\n",
        "    additional_options={\"dataQualityResultsPublishing.strategy\": \"BEST_EFFORT\", \"observations.scope\": \"ALL\"}\n",
        ")\n"
      ],
      "metadata": {
        "id": "qA3bpNa8gcr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the cleaned arrival data to S3 with Snappy compression.\n",
        "AmazonS3_node1744477762725 = glueContext.write_dynamic_frame.from_options(\n",
        "    frame=arrivals_convert_to_single_dynamic_frame_node1744477730878,\n",
        "    connection_type=\"s3\",\n",
        "    format=\"glueparquet\",\n",
        "    connection_options={\"path\": \"s3://airline-dataset-quaser/ad_cleaned_data/ad_arrivals/\", \"partitionKeys\": []},\n",
        "    format_options={\"compression\": \"snappy\"},\n",
        "    transformation_ctx=\"AmazonS3_node1744477762725\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "bw8XLTIvgeWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final job commit to ensure that all transformations and data writing steps are finalized and saved to S3.\n",
        "job.commit()\n"
      ],
      "metadata": {
        "id": "wXLxMbhSgfsc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}